{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.read_pickle(\"./dataset/prompts_3000.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_response(prompts, file_name):\n",
    "    chat_responses = []\n",
    "    chat_prompts = []\n",
    "    system_msg = \"You are a helpful assistant.\"\n",
    "    for prompt in prompts:\n",
    "        user_msg = prompt\n",
    "        response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg}]\n",
    "        )\n",
    "        chat_prompts.append(prompt)\n",
    "        chat_responses.append(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        time.sleep(15)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(chat_responses, f)\n",
    "\n",
    "    return chat_prompts, chat_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = generate_response(prompts[:2]) #first 2 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (p_0to100, r_0to100) = generate_response(prompts[:100], 'GPTresponse_0to100.pkl') #first 100 prompts and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(pd.read_pickle(\"GPTresponse_0to100.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p_100to500, r_100to500) = generate_response(prompts[100:500], 'GPTresponse_100to500.pkl') #100:500 prompts and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p_500to1000, r_500to1000) = generate_response(prompts[500:1000], 'GPTresponse_500to1000.pkl') #500:1000 prompts and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p_1000to1500, r_1000to1500) = generate_response(prompts[1000:1500], 'GPTresponse_1000to1500.pkl') #1000:1500 prompts and responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de891f2c564194911e369d0466d8269f0965ccdb3d788d267c2be370afd70219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
