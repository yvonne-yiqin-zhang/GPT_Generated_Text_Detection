{"cells":[{"cell_type":"markdown","metadata":{"id":"Nph-b1jG59tw"},"source":["# TF-IDF"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"PUBM--r959t1"},"outputs":[],"source":["import pickle\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"J-JiF9Eb59t3","outputId":"dae4eaa3-132f-4580-c112-780b9b35b86c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(3000, 2)\n","(300, 2)\n"]}],"source":["dataset_3000 = pd.read_pickle('./dataset/text_3000.pickle')\n","dataset_300 = pd.read_pickle('./dataset/text_300.pickle')\n","\n","print(dataset_3000.shape)\n","print(dataset_300.shape)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6__DuAql59t4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"logic of empire\" is a science fiction novel b...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>major general richard hutton davies,  (14 nove...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>elgin reptiles is the name given to a group of...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>dubgaill and finngaill, or dubgenti and finnge...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>chang teh-ming (; born 1938) is a taiwanese ph...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  \"logic of empire\" is a science fiction novel b...      1\n","1  major general richard hutton davies,  (14 nove...      1\n","2  elgin reptiles is the name given to a group of...      1\n","3  dubgaill and finngaill, or dubgenti and finnge...      0\n","4  chang teh-ming (; born 1938) is a taiwanese ph...      1"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset_3000.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"inRhqFDm59t5","outputId":"2686398a-4c4c-496a-ae9f-16a0c5002dbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["vocabulary size:  36862\n"]}],"source":["vectorizer = TfidfVectorizer()\n","vectorizer.fit(dataset_3000['text'])\n","print('vocabulary size: ', len(vectorizer.vocabulary_))\n","\n","vector_3000 = vectorizer.transform(dataset_3000['text'])\n","vector_300 = vectorizer.transform(dataset_300['text'])"]},{"cell_type":"markdown","metadata":{"id":"u6wsTNFR59t6"},"source":["Too many words, the vector is too long, we only use words with minimam frequency 15"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"0Jymab8859t7","outputId":"3212105d-ab1f-4f6d-8fdb-22af0e2e1289"},"outputs":[{"name":"stdout","output_type":"stream","text":["vocabulary size:  2815\n"]}],"source":["vectorizer = TfidfVectorizer(min_df=15)\n","vectorizer.fit(dataset_3000['text'])\n","print('vocabulary size: ', len(vectorizer.vocabulary_))\n","\n","vector_3000 = vectorizer.transform(dataset_3000['text'])\n","vector_300 = vectorizer.transform(dataset_300['text'])"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"QzZGaCxq59t9"},"outputs":[],"source":["dataset_3000_vector = pd.DataFrame(columns=['vector', 'label'])\n","dataset_3000_vector['vector'] = list(vector_3000.toarray())\n","dataset_3000_vector['label'] = dataset_3000['label']\n","\n","dataset_300_vector = pd.DataFrame(columns=['vector', 'label'])\n","dataset_300_vector['vector'] = list(vector_300.toarray())\n","dataset_300_vector['label'] = dataset_300['label']"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["dataset_3000_vector.to_pickle('./dataset/vector_3000_tfidf.pickle')\n","dataset_300_vector.to_pickle('./dataset/vector_300_tfidf.pickle')"]},{"cell_type":"markdown","metadata":{},"source":["# Read from pickle"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train set size:  3000\n","test set size:  300\n","train set vector size:  2815\n","test set vector size:  2815\n"]}],"source":["dataset_3000_vector = pd.read_pickle('./dataset/vector_3000_tfidf.pickle')\n","dataset_300_vector = pd.read_pickle('./dataset/vector_300_tfidf.pickle')\n","\n","print('train set size: ', len(dataset_3000_vector))\n","print('test set size: ', len(dataset_300_vector))\n","\n","print('train set vector size: ', len(dataset_3000_vector['vector'][0]))\n","print('test set vector size: ', len(dataset_300_vector['vector'][0]))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>vector</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[0.0, 0.12388382086309918, 0.0, 0.0, 0.0, 0.0,...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[0.0, 0.0, 0.0, 0.10454673097714223, 0.0, 0.0,...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              vector  label\n","0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      1\n","1  [0.0, 0.12388382086309918, 0.0, 0.0, 0.0, 0.0,...      1\n","2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      1\n","3  [0.0, 0.0, 0.0, 0.10454673097714223, 0.0, 0.0,...      0\n","4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      1"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset_3000_vector.head()"]},{"cell_type":"markdown","metadata":{},"source":["# For GPT 3.5"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(3000, 2)\n","(300, 2)\n"]}],"source":["dataset_3000 = pd.read_pickle('./dataset/text_3000_Turbo.pickle')\n","dataset_300 = pd.read_pickle('./dataset/text_300_Turbo.pickle')\n","\n","print(dataset_3000.shape)\n","print(dataset_300.shape)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["vocabulary size:  4199\n"]}],"source":["vectorizer = TfidfVectorizer(min_df=15)\n","vectorizer.fit(dataset_3000['text'])\n","print('vocabulary size: ', len(vectorizer.vocabulary_))\n","\n","vector_3000 = vectorizer.transform(dataset_3000['text'])\n","vector_300 = vectorizer.transform(dataset_300['text'])"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["dataset_3000_vector = pd.DataFrame(columns=['vector', 'label'])\n","dataset_3000_vector['vector'] = list(vector_3000.toarray())\n","dataset_3000_vector['label'] = dataset_3000['label']\n","\n","dataset_300_vector = pd.DataFrame(columns=['vector', 'label'])\n","dataset_300_vector['vector'] = list(vector_300.toarray())\n","dataset_300_vector['label'] = dataset_300['label']"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["dataset_3000_vector.to_pickle('./dataset/vector_3000_Turbo_tfidf.pickle')\n","dataset_300_vector.to_pickle('./dataset/vector_300_Turbo_tfidf.pickle')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"NLP","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
