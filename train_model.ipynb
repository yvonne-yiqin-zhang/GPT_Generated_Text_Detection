{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vector_to_columns(data):\n",
    "    col_names = ['feature'+str(i) for i in range(len(data.vector[0]))]\n",
    "    X = pd.DataFrame(data.vector.tolist(), columns=col_names)\n",
    "    return X, data.label.to_numpy()\n",
    "\n",
    "def data_preparation(data_name):\n",
    "    data = pd.read_pickle(data_name)\n",
    "    X, y = split_vector_to_columns(data)\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models_default_params = [LogisticRegression, RidgeClassifier, svm.SVC, SGDClassifier, Perceptron,\n",
    "                              GaussianNB, \n",
    "                              DecisionTreeClassifier,\n",
    "                              BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              KNeighborsClassifier,\n",
    "                              MLPClassifier]\n",
    "scoring = {'accuracy': 'accuracy',\n",
    "        'f1_score': 'f1',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_parameters = {LogisticRegression.__name__: {'max_iter': [600]},\n",
    "                Perceptron.__name__: {'alpha': [0.0001, 0.001, 0.01, 0.1]},\n",
    "                RidgeClassifier.__name__: {'alpha': [0.01, 0.05, 0.1, 0.5, 1, 5]},\n",
    "                svm.SVC.__name__: {'kernel': ['rbf', 'linear', 'poly']},\n",
    "                SGDClassifier.__name__: {'alpha': [0.0001, 0.001, 0.01]},\n",
    "                GaussianNB.__name__: {},\n",
    "                DecisionTreeClassifier.__name__: {'min_samples_split': [10, 20, 30], 'min_samples_leaf': [10, 20, 30]},\n",
    "                BaggingClassifier.__name__: {'n_estimators': [5,10,20]},\n",
    "                AdaBoostClassifier.__name__: {'n_estimators': [10,25,50,75], 'learning_rate': [0.1, 0.5, 1, 5]},\n",
    "                RandomForestClassifier.__name__: {'min_samples_split': [10, 20, 30], 'min_samples_leaf': [10, 20, 30]},\n",
    "                #GradientBoostingClassifier.__name__: {},\n",
    "                GradientBoostingClassifier.__name__: {'learning_rate': [0.01, 0.1, 0.5], 'min_samples_split': [5, 10], 'min_samples_leaf': [5, 10]},\n",
    "                KNeighborsClassifier.__name__: {'n_neighbors': [3, 5, 7]},\n",
    "                MLPClassifier.__name__: {'hidden_layer_sizes': [(100,), (100,100,), (100,10,)],'alpha': [0.0001, 0.001, 0.01]}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_performance(list_models_GS, X, y, scoring, cv=5, train=True, file_name=\"\", save_dir=None, verbose=True):\n",
    "    \"\"\"\n",
    "    For each model in the list_models, compute validation score for all metrics in scoring\n",
    "    For each model, return the average cross validated score for each metric\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    idx_names = []\n",
    "    tot = len(list_models_GS)\n",
    "    for i, model in enumerate(list_models_GS):\n",
    "        if verbose:\n",
    "            print(f\"Evaluating model {(i+1)}/{tot}: {type(model).__name__}...\")\n",
    "        idx_names.append(type(model).__name__)\n",
    "        scores = cross_validate(model, X, y, scoring=scoring, cv=cv, return_train_score=train)\n",
    "        for key, _ in scores.items():\n",
    "            score = scores[key].mean()\n",
    "            if key not in result:\n",
    "                result[key] = [score]\n",
    "            else: \n",
    "                result[key].append(score)\n",
    "    output = pd.DataFrame(result, index=idx_names)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
    "    save_dir = \".\" if save_dir is None else save_dir\n",
    "    output_file = os.path.join(save_dir, \"Exp_result-{}-cv={}.log\".format(file_name, cv))\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(output.to_string(index=True))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_GS(list_models, X, y, list_parameters, scoring = \"f1\", n_jobs=-1, cv=5, verbose=True):\n",
    "    \"\"\"\n",
    "    For each model in list_models, perform grid search and return the model instance initiated with the best performing parameter based on scoring\n",
    "    \"\"\"\n",
    "    def compute_GS_one_model(model, X, y, params, scoring = \"f1\", n_jobs=-1, cv=5):\n",
    "        grid= GridSearchCV(model, params, scoring = scoring, n_jobs=n_jobs, cv=cv)\n",
    "        grid.fit(X, y)\n",
    "        return grid.best_params_\n",
    "    \n",
    "    list_models_GS = []\n",
    "    tot = len(list_models)\n",
    "    for i, model in enumerate(list_models):\n",
    "        params = list_parameters[model.__name__]\n",
    "        if verbose:\n",
    "            print(f\"Grid search model {(i+1)}/{tot}: {model.__name__}...\")\n",
    "        if params != {}:\n",
    "            p = compute_GS_one_model(model(), X, y, params, scoring=scoring, n_jobs=n_jobs, cv=cv)\n",
    "            model_GS = model(**p)\n",
    "        else:\n",
    "            model_GS = model()\n",
    "        list_models_GS.append(model_GS)\n",
    "    return list_models_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# def compute_GS(list_models, X, y, list_parameters, scoring = \"accuracy\", cv=5):\n",
    "#     def compute_GS_one_model(model, X, y, params, scoring = \"accuracy\", cv=5):\n",
    "#         grid= GridSearchCV(model, params, scoring = scoring, cv=cv)\n",
    "#         grid.fit(X, y)\n",
    "#         return grid.best_params_\n",
    "    \n",
    "#     def compute_GS_one_model_wrapper(args):\n",
    "#         return compute_GS_one_model(*args)\n",
    "    \n",
    "#     pool = multiprocessing.Pool()\n",
    "#     results = []\n",
    "#     for model in list_models:\n",
    "#         param = list_parameters[model.__name__]\n",
    "#         if param != {}:\n",
    "#             args = (model(), X, y, param, scoring, cv)\n",
    "#             results.append(pool.apply_async(compute_GS_one_model_wrapper, (args,)))\n",
    "#         else:\n",
    "#             model_GS = model()\n",
    "#             results.append(model_GS)\n",
    "#     list_models_GS = []\n",
    "#     for result in results:\n",
    "#         if isinstance(result, multiprocessing.pool.ApplyResult):\n",
    "#             best_params = result.get()\n",
    "#             model_GS = model(**best_params)\n",
    "#         else:\n",
    "#             model_GS = result\n",
    "#         list_models_GS.append(model_GS)\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     return list_models_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = './experiment_results'\n",
    "CV = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: read small dataset\n",
      "Step 2: hyperparameter search\n",
      "Grid search model 1/13: LogisticRegression...\n",
      "Grid search model 2/13: RidgeClassifier...\n",
      "Grid search model 3/13: SVC...\n",
      "Grid search model 4/13: SGDClassifier...\n",
      "Grid search model 5/13: Perceptron...\n",
      "Grid search model 6/13: GaussianNB...\n",
      "Grid search model 7/13: DecisionTreeClassifier...\n",
      "Grid search model 8/13: BaggingClassifier...\n",
      "Grid search model 9/13: AdaBoostClassifier...\n",
      "Grid search model 10/13: RandomForestClassifier...\n",
      "Grid search model 11/13: GradientBoostingClassifier...\n",
      "Grid search model 12/13: KNeighborsClassifier...\n",
      "Grid search model 13/13: MLPClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression(max_iter=600), RidgeClassifier(alpha=5), SVC(), SGDClassifier(alpha=0.001), Perceptron(), GaussianNB(), DecisionTreeClassifier(min_samples_leaf=20, min_samples_split=10), BaggingClassifier(n_estimators=5), AdaBoostClassifier(learning_rate=0.5, n_estimators=25), RandomForestClassifier(min_samples_leaf=20, min_samples_split=30), GradientBoostingClassifier(learning_rate=0.5, min_samples_leaf=5,\n",
      "                           min_samples_split=10), KNeighborsClassifier(n_neighbors=7), MLPClassifier(alpha=0.01, hidden_layer_sizes=(100, 10))]\n",
      "Step 3: read large dataset and compute performance\n",
      "Evaluating model 1/13: LogisticRegression...\n",
      "Evaluating model 2/13: RidgeClassifier...\n",
      "Evaluating model 3/13: SVC...\n",
      "Evaluating model 4/13: SGDClassifier...\n",
      "Evaluating model 5/13: Perceptron...\n",
      "Evaluating model 6/13: GaussianNB...\n",
      "Evaluating model 7/13: DecisionTreeClassifier...\n",
      "Evaluating model 8/13: BaggingClassifier...\n",
      "Evaluating model 9/13: AdaBoostClassifier...\n",
      "Evaluating model 10/13: RandomForestClassifier...\n",
      "Evaluating model 11/13: GradientBoostingClassifier...\n",
      "Evaluating model 12/13: KNeighborsClassifier...\n",
      "Evaluating model 13/13: MLPClassifier...\n"
     ]
    }
   ],
   "source": [
    "#run sbert vectorization:\n",
    "SEARCH_FILENAME = \"./dataset/vector_300_sbert.pickle\"\n",
    "TRAIN_FILENAME = \"./dataset/vector_3000_sbert.pickle\"\n",
    "RESULT_NAME = TRAIN_FILENAME.split(\"/\")[2].split(\".\")[0]\n",
    "print(\"Step 1: read small dataset\")\n",
    "X, y = data_preparation(SEARCH_FILENAME)\n",
    "print(\"Step 2: hyperparameter search\")\n",
    "list_models_GS = compute_GS(list_models_default_params, X, y, list_parameters, cv=CV)\n",
    "print(list_models_GS)\n",
    "print(\"Step 3: read large dataset and compute performance\")\n",
    "X, y = data_preparation(TRAIN_FILENAME)\n",
    "r = compute_model_performance(list_models_GS, X, y, cv=CV, scoring=scoring, train=True, file_name=RESULT_NAME, save_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: read small dataset\n",
      "Step 2: hyperparameter search\n",
      "Grid search model 1/13: LogisticRegression...\n",
      "Grid search model 2/13: RidgeClassifier...\n",
      "Grid search model 3/13: SVC...\n",
      "Grid search model 4/13: SGDClassifier...\n",
      "Grid search model 5/13: Perceptron...\n",
      "Grid search model 6/13: GaussianNB...\n",
      "Grid search model 7/13: DecisionTreeClassifier...\n",
      "Grid search model 8/13: BaggingClassifier...\n",
      "Grid search model 9/13: AdaBoostClassifier...\n",
      "Grid search model 10/13: RandomForestClassifier...\n",
      "Grid search model 11/13: GradientBoostingClassifier...\n",
      "Grid search model 12/13: KNeighborsClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search model 13/13: MLPClassifier...\n",
      "[LogisticRegression(max_iter=600), RidgeClassifier(alpha=0.01), SVC(), SGDClassifier(alpha=0.001), Perceptron(), GaussianNB(), DecisionTreeClassifier(min_samples_leaf=10, min_samples_split=20), BaggingClassifier(n_estimators=5), AdaBoostClassifier(learning_rate=0.5, n_estimators=25), RandomForestClassifier(min_samples_leaf=10, min_samples_split=30), GradientBoostingClassifier(learning_rate=0.5, min_samples_leaf=10,\n",
      "                           min_samples_split=10), KNeighborsClassifier(n_neighbors=3), MLPClassifier(alpha=0.001, hidden_layer_sizes=(100, 100))]\n",
      "Step 3: read large dataset and compute performance\n",
      "Evaluating model 1/13: LogisticRegression...\n",
      "Evaluating model 2/13: RidgeClassifier...\n",
      "Evaluating model 3/13: SVC...\n",
      "Evaluating model 4/13: SGDClassifier...\n",
      "Evaluating model 5/13: Perceptron...\n",
      "Evaluating model 6/13: GaussianNB...\n",
      "Evaluating model 7/13: DecisionTreeClassifier...\n",
      "Evaluating model 8/13: BaggingClassifier...\n",
      "Evaluating model 9/13: AdaBoostClassifier...\n",
      "Evaluating model 10/13: RandomForestClassifier...\n",
      "Evaluating model 11/13: GradientBoostingClassifier...\n",
      "Evaluating model 12/13: KNeighborsClassifier...\n",
      "Evaluating model 13/13: MLPClassifier...\n"
     ]
    }
   ],
   "source": [
    "#run bow vectorization:\n",
    "SEARCH_FILENAME = \"./dataset/vector_300_bow.pickle\"\n",
    "TRAIN_FILENAME = \"./dataset/vector_3000_bow.pickle\"\n",
    "RESULT_NAME = TRAIN_FILENAME.split(\"/\")[2].split(\".\")[0]\n",
    "print(\"Step 1: read small dataset\")\n",
    "X, y = data_preparation(SEARCH_FILENAME)\n",
    "print(\"Step 2: hyperparameter search\")\n",
    "list_models_GS = compute_GS(list_models_default_params, X, y, list_parameters, cv=CV)\n",
    "print(list_models_GS)\n",
    "print(\"Step 3: read large dataset and compute performance\")\n",
    "X, y = data_preparation(TRAIN_FILENAME)\n",
    "r = compute_model_performance(list_models_GS, X, y, cv=CV, scoring=scoring, train=True, file_name=RESULT_NAME, save_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: read small dataset\n",
      "Step 2: hyperparameter search\n",
      "Grid search model 1/13: LogisticRegression...\n",
      "Grid search model 2/13: RidgeClassifier...\n",
      "Grid search model 3/13: SVC...\n",
      "Grid search model 4/13: SGDClassifier...\n",
      "Grid search model 5/13: Perceptron...\n",
      "Grid search model 6/13: GaussianNB...\n",
      "Grid search model 7/13: DecisionTreeClassifier...\n",
      "Grid search model 8/13: BaggingClassifier...\n",
      "Grid search model 9/13: AdaBoostClassifier...\n",
      "Grid search model 10/13: RandomForestClassifier...\n",
      "Grid search model 11/13: GradientBoostingClassifier...\n",
      "Grid search model 12/13: KNeighborsClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search model 13/13: MLPClassifier...\n",
      "[LogisticRegression(max_iter=600), RidgeClassifier(alpha=0.01), SVC(), SGDClassifier(alpha=0.001), Perceptron(), GaussianNB(), DecisionTreeClassifier(min_samples_leaf=20, min_samples_split=10), BaggingClassifier(n_estimators=5), AdaBoostClassifier(learning_rate=0.1, n_estimators=75), RandomForestClassifier(min_samples_leaf=30, min_samples_split=30), GradientBoostingClassifier(learning_rate=0.5, min_samples_leaf=10,\n",
      "                           min_samples_split=5), KNeighborsClassifier(), MLPClassifier(hidden_layer_sizes=(100, 10))]\n",
      "Step 3: read large dataset and compute performance\n",
      "Evaluating model 1/13: LogisticRegression...\n",
      "Evaluating model 2/13: RidgeClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.69254e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.45285e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.6695e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.45382e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.65081e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.50267e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.51276e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.56245e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model 3/13: SVC...\n",
      "Evaluating model 4/13: SGDClassifier...\n",
      "Evaluating model 5/13: Perceptron...\n",
      "Evaluating model 6/13: GaussianNB...\n",
      "Evaluating model 7/13: DecisionTreeClassifier...\n",
      "Evaluating model 8/13: BaggingClassifier...\n",
      "Evaluating model 9/13: AdaBoostClassifier...\n",
      "Evaluating model 10/13: RandomForestClassifier...\n",
      "Evaluating model 11/13: GradientBoostingClassifier...\n",
      "Evaluating model 12/13: KNeighborsClassifier...\n",
      "Evaluating model 13/13: MLPClassifier...\n"
     ]
    }
   ],
   "source": [
    "#run sent2vec vectorization:\n",
    "SEARCH_FILENAME = \"./dataset/vector_300_sent2vec.pickle\"\n",
    "TRAIN_FILENAME = \"./dataset/vector_3000_sent2vec.pickle\"\n",
    "RESULT_NAME = TRAIN_FILENAME.split(\"/\")[2].split(\".\")[0]\n",
    "print(\"Step 1: read small dataset\")\n",
    "X, y = data_preparation(SEARCH_FILENAME)\n",
    "print(\"Step 2: hyperparameter search\")\n",
    "list_models_GS = compute_GS(list_models_default_params, X, y, list_parameters, cv=CV)\n",
    "print(list_models_GS)\n",
    "print(\"Step 3: read large dataset and compute performance\")\n",
    "X, y = data_preparation(TRAIN_FILENAME)\n",
    "r = compute_model_performance(list_models_GS, X, y, cv=CV, scoring=scoring, train=True, file_name=RESULT_NAME, save_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: read small dataset\n",
      "Step 2: hyperparameter search\n",
      "Grid search model 1/13: LogisticRegression...\n",
      "Grid search model 2/13: RidgeClassifier...\n",
      "Grid search model 3/13: SVC...\n",
      "Grid search model 4/13: SGDClassifier...\n",
      "Grid search model 5/13: Perceptron...\n",
      "Grid search model 6/13: GaussianNB...\n",
      "Grid search model 7/13: DecisionTreeClassifier...\n",
      "Grid search model 8/13: BaggingClassifier...\n",
      "Grid search model 9/13: AdaBoostClassifier...\n",
      "Grid search model 10/13: RandomForestClassifier...\n",
      "Grid search model 11/13: GradientBoostingClassifier...\n",
      "Grid search model 12/13: KNeighborsClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n",
      "/Users/yyz/Desktop/myproject/gpt_speech_detection_./venv/lib/python3.8/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.8 is maybe too old for this OS.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search model 13/13: MLPClassifier...\n",
      "[LogisticRegression(max_iter=600), RidgeClassifier(alpha=0.01), SVC(kernel='linear'), SGDClassifier(alpha=0.001), Perceptron(), GaussianNB(), DecisionTreeClassifier(min_samples_leaf=30, min_samples_split=10), BaggingClassifier(n_estimators=20), AdaBoostClassifier(learning_rate=1), RandomForestClassifier(min_samples_leaf=10, min_samples_split=10), GradientBoostingClassifier(learning_rate=0.5, min_samples_leaf=10,\n",
      "                           min_samples_split=5), KNeighborsClassifier(n_neighbors=7), MLPClassifier(hidden_layer_sizes=(100, 100))]\n",
      "Step 3: read large dataset and compute performance\n",
      "Evaluating model 1/13: LogisticRegression...\n",
      "Evaluating model 2/13: RidgeClassifier...\n",
      "Evaluating model 3/13: SVC...\n",
      "Evaluating model 4/13: SGDClassifier...\n",
      "Evaluating model 5/13: Perceptron...\n",
      "Evaluating model 6/13: GaussianNB...\n",
      "Evaluating model 7/13: DecisionTreeClassifier...\n",
      "Evaluating model 8/13: BaggingClassifier...\n",
      "Evaluating model 9/13: AdaBoostClassifier...\n",
      "Evaluating model 10/13: RandomForestClassifier...\n",
      "Evaluating model 11/13: GradientBoostingClassifier...\n",
      "Evaluating model 12/13: KNeighborsClassifier...\n",
      "Evaluating model 13/13: MLPClassifier...\n"
     ]
    }
   ],
   "source": [
    "#run tfidf vectorization:\n",
    "SEARCH_FILENAME = \"./dataset/vector_300_tfidf.pickle\"\n",
    "TRAIN_FILENAME = \"./dataset/vector_3000_tfidf.pickle\"\n",
    "RESULT_NAME = TRAIN_FILENAME.split(\"/\")[2].split(\".\")[0]\n",
    "print(\"Step 1: read small dataset\")\n",
    "X, y = data_preparation(SEARCH_FILENAME)\n",
    "print(\"Step 2: hyperparameter search\")\n",
    "list_models_GS = compute_GS(list_models_default_params, X, y, list_parameters, cv=CV)\n",
    "print(list_models_GS)\n",
    "print(\"Step 3: read large dataset and compute performance\")\n",
    "X, y = data_preparation(TRAIN_FILENAME)\n",
    "r = compute_model_performance(list_models_GS, X, y, cv=CV, scoring=scoring, train=True, file_name=RESULT_NAME, save_dir=SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de891f2c564194911e369d0466d8269f0965ccdb3d788d267c2be370afd70219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
